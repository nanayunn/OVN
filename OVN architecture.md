## OVN architecture



* ovn이란?
  *  가상 시스템 및 컨테이너 환경에서 논리적 네트워크 추상화를 지원하는 시스템
  * 논리적 L2, L3 오버레이 네트워크나 보안그룹과 같은 논리적 네트워크 추상화에 대한 기본 지원을 추가하였다.
    * => 현 OVS 기능을 보완한다.
* OVN의 설계 목표
  * OVS와 마찬가지로, 상당한 규모를 운용할 수 있는 생산 품질을 구현해내는 것 이다.



* 물리 네트워크
  *  물리 와이어, 스위치, 라우터로 구성된다.
* 가상 네트워크
  * 이러한 물리 네트워크를 하이퍼 바이저와 컨테이너 플랫폼으로 확장
  *  vm와 컨테이너들을 물리 네트워크와 연결한다.
* ovn의 논리적 네트워크
  *  tunnel이나 캡슐화를 이용하여 물리 네트워크로부터 격리된 소프트웨어에서 실행되는 네트워크
    * 논리적 네트워크에서 사용되는 IP와 주소공간이 충돌없이 물리네트워크에서 쓰이는 IP와 주소공간과 겹칠 수 있도록 한다.
  * 논리적 네트워크 토폴로지는 물리 네트워크 토폴로지를 고려하지 않고 배열할 수 있다.
  * 따라서 논리적 네트워크의 일부가 되는 VM은 네트워크 중단 없이 물리 머신 간의 이동이 가능해진다.



* 캡슐화 레이어
  * 논리 네트워크에 연결된 vm과 컨테이너들이 물리 네트워크 상의 노드들과 통신하지 않도록 한다.
  * vm과 컨테이너를 클러스터링하는 경우 캡슐화 레이어의 이용은 허용 가능하며, 더 나아가 아주 바람직하다고 볼 수 있지만, 대부분의 많은 경우에 vm과 컨테이너들은 물리 네트워크와의 연결을 필요로 한다. 
  * ovn은 이런 경우에 대한 해결책으로 다중 폼의 gateway를 지원한다. 



* ovn 배포는 다음과 같은 몇 가지 요소들로 구성된다.
  * CMS(cloud Management System)
    * OVN의 최종 클라이언트( 사용자나 관리자를 통한 )
    * OVN 통합을 수행하기 위해서는 CMS별 플러그인과 관련 소프트웨어를 설치해아한다.
    * OVN 초기 설정은  Openstack을 CMS로 삼는다.
    * 일반적으로 CMS를 통칭할 때 하나의 CMS만을 고려하지만, 각각의 ovn 배포 구역을 담당하는 여러개의 CMS가 설치되어있는 시나리오 또한 생각해 볼 수 있을 것이다.
  * OVN 데이터베이스를 구성하는 물리/가상 노드 혹은 클러스터
    * 가장 중심지역에 설치되어 있어야 한다.
  * 1개 이상의 하이퍼바이저
    * 필수 설치 사항
      * Open vSwitch가 실행되어야한다.
      * OVN 소스 트리의 Documentation/topics/integration.rst에 설명되어 있는 인터페이스를 구현해야 한다.
      * Open vSwitch가 지원하는 모든 하이퍼 바이저 플랫폼이 이용 가능하다.
  * 0개 이상의 gateway
    * 게이트웨이는 터널과 물리적 이더넷 포트 간에 양방향으로 패킷을 전달하여 터널 기반 논리적 네트워크를 물리적 네트워크로 확장한다.
      *  이를 통해 가상화되지 않은 시스템도 논리 네트워크에 포함될 수 있도록 해준다.
      * 게이트웨이는 vtep(5) 스키마를 지원하는 물리적 호스트, 가상 시스템 또는 ASIC 기반 하드웨어 스위치일 수 있다.
  * 하이퍼바이저 + 게이트웨이 =  transport node 또는 chassis라고 통칭한다.



* 아래 다이어그램은 OVN의 주요 구성 요소와 관련 소프트웨어의 상호 작용 방식을 보여줍니다. 

* 이 다이어그램의 맨 위에서 시작하여 다음과 같은 기능들이 있습니다.

  * 위에서 정의 된 CMS 관련

  * OVN/CMS 플러그인

    * OVN에 연결되는 CMS의 구성 요소
      * OpenStack에서는 Neutron 플러그인
    * 플러그인의 주요 목적
      * CMS의 구성 데이터베이스에 저장된 CMS의 논리적 네트워크 구성에 대한 CMS의 개념을 OVN에서 이해하는 중간 표현으로 변환하는 것
      * 해당 요소들은 CMS마다 필수적이므로 OVN에 통합될 각각의 CMS에 대한 플러그인이 개발될 필요가 있다.
      * 글 하단의 다이어그램에 포함된 모든 구성요소들은 CMS로부터 독립적인 특성을 가지고 있다.

  * OVN Northbound Database 

    * OVN/CMS 플러그인에 의해 전달된 논리적 네트워크 구성의 중간 표현을 수신
      * 중간 표현?
        *  소스 코드를 나타 내기 위해 컴파일러 또는 가상 머신이 내부적으로 사용하는 데이터 구조 또는 코드
    * 데이터베이스 스키마는 CMS에서 사용되는 개념과 "임피던스 일치", 즉 동일한 선상에 있도록 되어 있으므로 논리적 스위치, 라우터, ACL 등의 개념을 크게 구분할 필요없이 이용할 수 있다.
    * 이 데이터베이스는 오로지 두 가지의 클라이언트만을 가진다.
      * OVN/CMS 플러그인
      * ovn-northd

  * ovn-northd

    * OVN Northbound Database와 OVN Southbound Database와 연결된다.
    * OVN Northbound Database에서 가져온 기존 네트워크 개념의 논리적 네트워크 구성을 그 아래 OVN Southbound Database의 논리적 데이터패스 흐름으로 변환합니다.

  * OVN Southbound

    * 시스템의 핵심
    * ovn-northd와  ovn-controller와 연결된다. 
    * 데이터베이스에 포함되는 데이터 3가지
      * Physical Network(PN) tables
        * 하이퍼바이저와 각각의 노드에 어떻게 도달할 수 있는지 구체화
      * Logical Network(LN) tables
        * "논리적 데이터패스 흐름" 측면에서 논리 네트워크를 설명
      * Binding tables
        * 논리 네트워크 구성요소들의 위치를 물리 네트워크와 연결
    * PN과 Port_Binding table 는 하이퍼바이저가 채우고, LN tables는 ovn-northd가 채운다.

  * OVN Southbound Database 성능

    *  전송 노드 수에 따라 확장이 필요

      *  병목 현상이 발생할 수 있으므로 ovsdb-server(1)에 대한 작업이 필요할 수 있다.
      *  가용성을 위한 클러스터링이 필요할 수 있다.

      

* 이외의 구성요소들은 각 하이퍼바이저마다 동일하게 복제되는 요소들이다.

  * ovn-controller
    * 각 하이퍼바이저와 소프트웨어 gateway에 설치되는 OVN agent
    * Northbound
      * OVN Southbound 데이터베이스에 연결하여 OVN 구성 및 상태에 대해 확인
      * 바인딩 테이블의 PN 테이블과 Chassis 열을 하이퍼바이저 상태 정보로 채움
    * Southbound
      *  네트워크 트래픽 제어를 위해 OpenFlow 컨트롤러로써 ovs-vswitchd(8)에 연결
      * 로컬 ovsdb-server(1)에 연결하여 Open vSwitch 구성을 모니터링 및 제어할 수 있도록 함
      * ovs-vswitchd와 ovsdb-server는 Open vSwitch의 기본적인 구성요소들이다.

***

### OVN의 information flow



OVN의 구성 데이터는 위(North)에서 아래(South)로 흐릅니다. 

CMS는 OVN/CMS 플러그인을 통해 논리적 네트워크 구성을 northbound 데이터베이스를 통해 ovn-northd로 전달합니다. 

그 다음, ovn-northd는 구성을 lower-level로 컴파일하고 이를  southbound 데이터베이스를 통해 모든 chassis로 전달합니다.

OVN의 상태 정보는 아래(South)에서 위(North)로 흐릅니다. 

OVN은 현재 몇 가지 형태의 상태 정보만 제공합니다. 

먼저, ovn-northd는 북쪽 방향 Logical_Switch_Port 테이블에 up 열을 채웁니다. 

남쪽 방향 Port_Binding 테이블에 있는 논리적 포트의 chassis 열이 비어 있지 않으면 true로 설정되고 그렇지 않으면 false로 설정됩니다. 

이를 통해 CMS는 VM의 네트워킹이 시작된 시기를 감지할 수 있습니다.

둘째, OVN은 CMS에서 제공한 구성이 적용되었는지 여부에 대한 CMS에 피드백을 제공합니다. 

이 기능을 사용하려면 CMS가 다음 방법으로 작동하는 시퀀스 번호 프로토콜에 참여해야 합니다.

* 1. CMS가 northbound 데이터 베이스의 설정을 업데이트할 때, CMS가 동일한 트랜잭션의 일부로 NB_Global 테이블에서 Nb_cfg 열의 값을 증가시킵니다. (이 절차는 CMS가 구성이 적용된 시기를 알고자 하는 경우에만 필요합니다.)
  2. ovn-northd는 지정된 northbound 데이터 베이스 스냅샷을 기반으로 southbound 데이터베이스를 업데이트하면,
     *  동일한 트랜잭션의 일부로 nb_cfg를 northbound NB_Global에서 southbound 데이터베이스 SB_Global 테이블로 복사합니다. (따라서, 두 데이터베이스를 모두 모니터링하는 관찰자는 남쪽 데이터베이스가 북쪽 데이터베이스로 이동하는 시기를 결정할 수 있습니다.)
  3. ovn-northd는 southbound 데이터베이스 서버로부터 변경사항이 커밋되었음을 확인한 후 northbound NB_Global 테이블의 sb_cfg를 푸시된 nb_cfg 버전으로 업데이트합니다. (Thus, CMS 또는 다른 관찰자는 southbound 데이터베이스에 연결하지 않고 southbound 데이터베이스가 검색되는 시기를 결정할 수 있습니다.)
  4. 각 chassis의 ovn-컨트롤러 프로세스는 업데이트된 nb_cfg 데이터베이스와 함께 업데이트된 남향 데이터베이스 받습니다. 이 프로세스는 chassis의 Open vSwitch 인스턴스에 설치된 물리적 흐름을 업데이트합니다. Open vSwitch로부터 물리적 흐름이 업데이트되었다는 확인을 수신하면 southbound 데이터베이스의 자체 chassis 레코드에서 Nb_cfg를 업데이트한다.
  5. ovn-northd는 southbound 데이터베이스의 모든 chassis 레코드에 있는 nb_cfg 열을 모니터링합니다. 모든 레코드 중 최소값을 추적하고 NB_Global 테이블의 hv_cfg 열에 복사합니다. (Thus, CMS 또는 다른 관찰자는 모든 하이퍼바이저가 북쪽 방향 구성을 따라잡은 시기를 결정할 수 있습니다.)



### Chassis setup

OVN 배포의 각 chassis는 OVN 전용 Open vSwitch 브리지(통합 브리지)로 구성해야 합니다. 원하는 경우 시스템 시작 스크립트가 ovn 컨트롤러를 시작하기 전에 이 브리지를 만들 수 있습니다. ovn 컨트롤러가 시작될 때 이 브리지가 없으면 아래에 제시된 기본 구성으로 자동으로 생성됩니다. 통합 브리지의 포트는 다음과 같습니다.

* 모든 chassis에서 OVN이 논리적 네트워크 연결을 유지하는 데 사용하는 터널 포트입니다. ovn은 이러한 터널 포트를 추가, 업데이트 및 제거합니다.
* 하이퍼바이저에서 논리적 네트워크에 연결할 모든 VIF입니다. 
  * 하이퍼바이저 자체 또는 Open vSwitch와 하이퍼바이저 간의 통합(Documentation/topics/integration.rst에 설명되어 있음)이 이것을 관리합니다. (이 작업은 OVN의 일부이거나 OVN의 새로운 기능이 아니며 OVS를 지원하는 하이퍼바이저에서 이미 수행된 기존 통합 작업입니다.)
* 게이트웨이에서 논리적 네트워크 연결에 사용되는 물리적 포트입니다. 시스템 시작 스크립트는 ovn 컨트롤러를 시작하기 전에 브리지에 이 포트를 추가합니다. 이는 보다 정교한 설정으로 물리적 포트를 대신하는, 다른 브리지에 대한 패치 포트일 수 있습니다.



다른 포트는 통합 브리지에 연결하면 안 됩니다. 

특히 언더레이 네트워크에 연결된 물리적 포트(논리 네트워크에 연결된 물리적 포트인 게이트웨이 포트와는 달리)를 통합 브리지에 연결해서는 안 됩니다. 언더레이 물리적 포트는 대신 별도의 Open vSwitch 브리지에 연결해야 합니다(사실상 브리지에는 전혀 연결할 필요가 없습니다).

통합 브리지는 아래 설명과 같이 구성해야 합니다. 이러한 각 설정의 효과는 ovs-vswitchd.conf.db(5)에 설명되어 있습니다.

* fail−mode=secure
  * av는 ovn 컨트롤러가 시작되기 전에 분리된 논리적 네트워크 간에 패킷을 전환하는 것을 지원합니다. 자세한 내용은 ovs-vsctl(8)의 컨트롤러 장애 설정을 참조합니다.
* other−config:disable−in−band=true
  * 통합 브리지에 대한 대역 내 제어 흐름을 억제합니다. OVN은 원격 컨트롤러 대신 로컬 컨트롤러(오버아유닉스 도메인 소켓)를 사용하기 때문에 이러한 흐름이 나타나는 것은 드문 일입니다. 그러나 동일한 시스템의 다른 브리지에는 인밴드 원격 컨트롤러가 있을 수 있으며, 이 경우 인밴드 컨트롤이 일반적으로 설정하는 흐름을 억제합니다. 자세한 내용은 설명서를 참조합니다.



통합 브리지의 일반적인 이름은 br-int이지만 다른 이름을 사용할 수 있습니다.

***

### Logical Networks



OVN의 논리적 네트워크 개념에는 각각 이더넷 스위치와 IP 라우터의 논리적 버전인 논리적 스위치와 논리적 라우터가 포함됩니다. 물리적 사촌과 마찬가지로 논리적 스위치와 라우터는 정교한 토폴로지로 연결될 수 있습니다. 논리적 스위치 및 라우터는 일반적으로 순수하게 논리적 엔티티이며, 즉, 물리적 위치에 연결되거나 바인딩되지 않으며 OVN에 참여하는 각 하이퍼바이저에서 분산된 방식으로 구현됩니다. 

LSP(논리 스위치 포트)는 논리적 스위치 내부 및 외부 연결 지점입니다. 논리적 스위치 포트는 여러 종류가 있습니다. 가장 일반적인 유형은 VM 또는 컨테이너에 대한 VIF, 즉 연결 지점을 나타냅니다. VIF 논리 포트는 VM의 물리적 위치와 연결되며, VM이 마이그레이션될 때 변경될 수 있습니다. VIF 논리 포트는 전원이 꺼지거나 일시 중단된 VM과 연결할 수 있습니다. 이러한 논리 포트는 위치와 연결성이 없습니다.)

논리적 라우터 포트(LRP)는 논리적 라우터 내부 및 외부 연결 지점입니다. LRP는 논리적 라우터를 논리적 스위치 또는 다른 논리적 라우터에 연결합니다. 논리적 라우터는 논리적 스위치를 통해 VM, 컨테이너 및 기타 네트워크 노드에만 간접적으로 연결합니다.

논리적 스위치와 논리적 라우터는 서로 다른 종류의 논리적 포트를 가지고 있으므로 적절하게 말하면 대개 논리적 스위치 포트 또는 논리적 라우터 포트에 대해 이야기해야 합니다. 그러나 부적격의 "논리적 포트"는 대개 논리적 스위치 포트를 가리킵니다. 

VM이 VIF 논리적 스위치 포트로 패킷을 보낼 때 Open vSwitch 흐름 테이블은 해당 논리적 스위치와 마주칠 수있는 다른 논리적 라우터 및 논리적 스위치를 통한 패킷의 이동을 시뮬레이션합니다. 이러한 현상은 물리적 매체로 패킷을 전송하지 않고 발생합니다. 플로우 테이블은 모든 스위칭 및 라우팅 결정과 동작을 구현합니다. 플로우 테이블이 궁극적으로 다른 하이퍼바이저(또는 다른 종류의 전송 노드)에 연결된 논리 포트에서 패킷을 출력하기로 결정한다면, 바로 이 순간이 패킷이 물리적 네트워크 전송을 위해 캡슐화되어 전송되는 시간입니다.



#### Logical Switch Port Types

OVN은 다양한 종류의 논리적 스위치 포트를 지원합니다. 위에서 설명한 VM 또는 컨테이너에 연결하는 VIF 포트는 가장 일반적인 종류의 LSP입니다. OVN 북향 데이터베이스에서 VIF 포트 유형에 대한 문자열이 비어 있습니다. 이 섹션에서는 일부 추가 포트 유형에 대해 설명합니다.

라우터 논리적 스위치 포트는 논리적 스위치를 논리적 라우터에 연결하여 특정 LRP를 피어로 지정합니다.

localnet 논리적 스위치 포트는 논리적 스위치를 물리적 VLAN에 브리지합니다. localnet LSP가 있는 논리적 스위치에는 다른 LSP가 하나만 있어야 합니다. 일부 게이트웨이(아래 게이트웨이 참조)에서는 라우터 포트가 있는 논리적 스위치를 두 번째 LSP로 사용합니다. 반면, 두 번째 LSP가 VIF인 경우 논리적 스위치는 절연되지 않고 물리적 네트워크에 연결되므로 논리적 스위치는 실제로 논리적 네트워크가 아닙니다. 따라서 독립적이지만 중복된 IP 주소 네임스페이스 등을 가질 수 없습니다. (그럼에도 불구하고 배포는 OVN 제어부 및 포트 보안 및 ACL과 같은 기능을 활용하기 위해 이러한 구성을 선택할 수 있습니다.)

localport 논리적 스위치 포트는 특별한 종류의 VIF 논리적 스위치 포트입니다. 이러한 포트는 특정 chassis에 바인딩되지 않고 모든 chassis에 있습니다. 이러한 포트로의 트래픽은 터널을 통해 전달되지 않으며, 이러한 포트의 트래픽은 일반적으로 수신된 요청에 따라 동일한 chassis로만 전달될 것으로 예상됩니다. OpenStack Neutron은 로컬 포트 포트를 사용하여 VM에 메타데이터를 제공합니다. 메타데이터 프록시 프로세스가 모든 호스트의 이 포트에 연결되고, 트래픽이 오버터널로 전송될 필요 없이 동일한 네트워크 내의 모든 VM이 동일한 IP/MAC 주소에 도달할 수 있습니다. 자세한 내용은 Networking-ovn용 OpenStack 설명서를 참조하시기 바랍니다.

LSP 유형 vtep 및 l2게이트웨이는 게이트웨이에 사용됩니다. 자세한 내용은 아래 게이트웨이를 참조합니다.



#### Implementation Details

이러한 개념은 OVN이 내부적으로 구현되는 방법에 대한 세부 사항입니다. 사용자 및 관리자에게 여전히 관심받는 분야입니다.

논리적 데이터패스는 OVN southbound 데이터베이스에 있는 논리적 네트워크의 구현 세부사항입니다. ovn-northd는 Northbound 데이터베이스의 각 논리적 스위치 또는 라우터를 southbound 데이터베이스 Datapath_Binding 테이블의 논리적 데이터 경로로 변환합니다.

 대부분의 경우, ovn-northd는 OVN Northbound 데이터베이스의 각 논리적 스위치 포트를 southbound 데이터베이스 Port_Binding 테이블의 레코드로 변환합니다. 후자 테이블은 대략 Northbound Logical_Switch_Port 표에 해당합니다. 여기에는 여러 유형의 논리적 포트 바인딩이 있으며, 이 중 많은 유형은 Northbound LSP 유형에 직접 대응됩니다. 이 방식으로 처리되는 LSP 유형에는 VIF(빈 문자열), localnet, localport, vtep 및 l2gateway가 포함됩니다. 

Port_Binding 테이블에는 논리적 스위치 포트 유형과 직접 일치하지 않는 일부 포트 바인딩 유형이 있습니다. 일반적인 공통점은 논리적 패치 포트로 알려진 패치 포트 바인딩입니다. 이러한 포트 바인딩은 항상 쌍으로 이루어지며 한쪽으로 들어오는 패킷은 다른 쪽에서 나옵니다. ovn-northd는 논리적 패치 포트를 사용하여 논리적 스위치와 논리적 라우터를 함께 연결합니다.
vtep, l2gateway, l3gateway 및 chassis 리디렉션 유형의 포트 바인딩이 게이트웨이에 사용됩니다. 이것들은
아래의 게이트웨이에 설명되어 있습니다.

***

### Gateway

게이트웨이는 논리적 네트워크와 물리적 네트워크 간에 제한된 연결을 제공합니다. OVN은 다양한 종류의 게이트웨이를 지원합니다.

#### VTEP Gateways

"VTEP 게이트웨이"는 OVN 논리적 네트워크를 Open vSwitch에 수반되는 OVSDB VTEP 스키마를 구현하는 물리적(또는 가상) 스위치에 연결합니다. ("VTEP 게이트웨이" 용어는 VTEP가 VXLAN Tunnel Endpoint(끝점)일 뿐이라는 점에서 부적절한 명칭으로 보일 수 있으나, 제대로 설정된 이름입니다. 자세한 내용은 아래 VTEP 게이트웨이의 수명 주기를 참조합니다. 
VTEP 게이트웨이의 주요 사용 사례는 OVSDB VTEP 스키마를 지원하는 물리적 TOR 스위치를 사용하여 물리적 서버를 OVN 논리적 네트워크에 연결하는 것입니다.



#### L2 Gateways

L2 게이트웨이는 일부 chassis에서 사용할 수 있는 지정된 물리적 L2 세그먼트를 논리 네트워크에 연결하기만 하면 됩니다. 물리적 네트워크는 효과적으로 논리적 네트워크의 일부가 됩니다.
L2 게이트웨이를 설정하려면,  CMS가 적절한 논리적 스위치에 l2게이트웨이 LSP를 추가하고 LSP 옵션을 설정하여 바인딩해야하는 chassis의 이름을 지정합니다. ovn-northd는 이 구성을 southbound Port_Binding 레코드에 복사합니다. 지정된 chassis에서 ovn 컨트롤러는 패킷을 물리적 세그먼트로 적절히 전달합니다.
L2 게이트웨이 포트는 localnet 포트와 공통적인 기능을 가지고 있습니다. 그러나 localnet 포트를 사용하면 물리적 네트워크가 하이퍼바이저 간의 전송 수단이 됩니다. L2 게이트웨이를 사용하면 터널을 통해 하이퍼바이저 간에 패킷이 여전히 전송될 수 있고, L2게이트웨이 포트는 물리적 네트워크에 있는 패킷에만 사용될 수 있게 됩니다. L2 게이트웨이 애플리케이션은 예를 들어, 가상화되지 않은 시스템을 논리적 네트워크에 추가하는 애플리케이션이란 점에서 VTEP 게이트웨이 애플리케이션과 유사하지만, VTEP 게이트웨이와 달리 L2 게이트웨이에는 TOR 하드웨어 스위치의 특별한 지원이 필요하지 않습니다.



#### L3 Gateway Routers

위에서 설명한 것처럼 일반 OVN 논리적 라우터는 분산되어있습니다. 즉, 한 곳에 구현되지 않고 모든 하이퍼바이저 chassis에 구현됩니다. 이는 중앙 집중식으로 구현해야 하는, SNAT, DNAT과 같은 Stateful 서비스에는 단점으로 작동합니다. 

이러한 종류의 기능을 지원하기 위해, OVN은 지정된 섀시에 구현된 OVN 논리적 라우터인 L3 게이트웨이 라우터를 지원합니다. 게이트웨이 라우터는 일반적으로 분산 논리적 라우터와 물리적 네트워크 사이에 사용됩니다. VM과 컨테이너가 연결되는 분산 논리적 라우터와 그 뒤의 논리적 스위치는 각 하이퍼바이저에 효과적으로 상주합니다.. 분산 라우터와 게이트웨이 라우터는 "조인" 논리적 스위치"라고도 하는 다른 논리적 스위치로 연결되어있습니다. (OVN 논리적 라우터는 개입 스위치 없이 서로 직접 연결할 수 있지만 OVN 구현에서는 논리적 스위치에 연결된 게이트웨이 논리적 라우터만 지원합니다. 조인 논리적 스위치를 사용하면 분산 라우터에 필요한 IP 주소 수도 줄어듭니다.) 한편으로, 게이트웨이 라우터는 물리적 네트워크에 연결되는 로컬 네트워크 포트가 있는 다른 논리적 스위치에 연결합니다.
다음 다이어그램은 일반적인 상황을 보여줍니다. 하나 이상의 논리적 스위치 LS1, ..., LSn은 분산 논리적 라우터 LR1에 연결되며, LSjoin을 통해 게이트웨이 논리적 라우터 GLR에도 연결되며, 물리적 네트워크에 연결할 localnet 포트가 포함된 논리적 스위치 LSlocal에도 연결됩니다.

L3 게이트웨이 라우터를 구성하기 위해 CMS는 라우터의 북쪽 바운드 Logical_Router에 있는 chassis 옵션을 chassis 이름으로 설정합니다. 이에 대해 ovn-northd는 패치 바인딩 대신 southbound database의 특수 L3게이트웨이 포트 바인딩을 사용하여 논리적 라우터를 인접 네트워크에 연결합니다. 그 다음, ovn 컨트롤러는 패킷을 로컬로 처리하는 대신 지정된 L3 게이트웨이 chassis에 바인딩된 포트에 패킷을 터널링합니다. 

DNAT 및 SNAT 규칙은 일대다 SNAT(IP 위장이라고도 함)를 처리할 수 있는 중앙 위치를 제공하는 게이트웨이 라우터에 연결할 수 있습니다.



#### Distributed Gateway Ports

분산 게이트웨이 포트는 중앙 집중식 처리를 위해 게이트웨이 섀시라고 하는 하나의 고유 섀시를 지정하도록 특별히 구성된 논리적 라우터 포트입니다. 분산 게이트웨이 포트는 localnet 포트를 사용하여 논리적 스위치에 연결해야 합니다. 분산 게이트웨이로 들어오고 나가는 패킷은 가능한 경우 게이트웨이 섀시를 포함하지 않고 처리되지만 필요할 때는 추가 홉을 통해 처리됩니다.
다음 다이어그램은 분산 게이트웨이 포트의 사용을 보여 줍니다. 여러 논리적 스위치 LS1, ..., LSn은 분산 논리적 라우터 LR1에 연결되며, LSn은 물리적 네트워크에 연결할 localnet 포트가 포함된 논리적 스위치 LSlocal에 차례로 연결됩니다.



ovn-northd는 두 개의 남쪽 방향 포트_Binding 레코드를 생성하여 일반적인 게이트웨이 포트가 아닌 분산 게이트웨이 포트를 나타냅니다. 그 중 하나는 가능한 많은 트래픽에 사용되는 LRP를 위해 명명된 패치 포트 바인딩입니다. 다른 하나는 cr-port라는 이름의 섀시 리디렉션 유형으로 포트 바인딩입니다. 섀시 리디렉션 포트 바인딩에는 패킷이 해당 바인딩으로 출력될 때 흐름 테이블이 해당 바인딩을 다음으로 터널링하도록 하는 하나의 특수 작업이 있습니다.

즉, 패치 포트 바인딩으로 자동 출력되는 게이트웨이 섀시입니다. 따라서 게이트웨이 섀시에서 특정 작업을 수행해야 하는 경우 흐름 테이블을 이 포트 바인딩으로 출력할 수 있습니다. 섀시 리디렉션 포트 바인딩은 다른 방법으로 사용되지 않습니다(예: 패킷을 수신하지 않음). 

CMS는 세 가지 방법으로 분산 게이트웨이 포트를 구성할 수 있습니다. 자세한 내용은 ovn-nb(5)의 Logical_Router_Port 설명서에서 분산 게이트웨이 포트를 참조합니다.
분산 게이트웨이 포트는 고가용성을 지원합니다. 둘 이상의 섀시를 지정한 경우 OVN은 한 번에 하나씩만 게이트웨이 섀시로 사용합니다. OVN은 BFD를 사용하여 게이트웨이 연결을 모니터링하며 온라인 상태의 가장 높은 우선순위 게이트웨이를 선호합니다.

***



### Life Cycle of a VIF

테이블과 테이블의 스키마는 따로따로 표시되므로 이해하기 어렵습니다. 여기 예가 있습니다.
하이퍼바이저의 VIF는 VM 또는 해당 하이퍼바이저에서 직접 실행되는 컨테이너에 연결된 가상 네트워크 인터페이스입니다(VM 내에서 실행되는 컨테이너의 인터페이스와는 다름). 
이 예제의 단계는 종종 OVN 및 OVN Northbound 데이터베이스 스키마에 대한 세부 정보를 참조합니다. 이러한 데이터베이스에 대한 자세한 내용은 ovn-sb(5) 및 ovn-nb(5)를 참조하시기 바랍니다.

1. VIF의 수명 주기는 CMS 관리자가 CMS 사용자 인터페이스 또는 API를 사용하여 새 VIF를 생성하고 이를 스위치(OVN에서 논리적 스위치로 구현한 VIF)에 추가할 때 시작됩니다. CMS가 자체 구성을 업데이트합니다. 여기에는 고유하고 영구적인 식별자 vif-id 및 이더넷 주소 mac을 VIF와 연결하는 작업이 포함됩니다.
2. CMS 플러그인은 논리적_Switch_Port 테이블에 행을 추가하여 OVN Northbound 데이터베이스를 업데이트하여 새 VIF를 포함합니다. 새 행에서 이름은 vif-id, mac은 mac, 스위치 OVN 논리적 스위치의 Logical_Switch 레코드를 가리키고 다른 열이 적절하게 초기화됩니다.
3. ovn-northd는 OVN Northbound 데이터베이스 업데이트를 수신합니다. OVN Southbound 데이터베이스 Logical_에 행을 추가하여 OVN Southbound 데이터베이스에 해당하는 업데이트를 수행합니다.새 포트를 반영하는 흐름 테이블(예: 새 포트의 MAC 주소로 향하는 패킷이 해당 포트에 전달되어야 함을 인식하는 흐름을 추가하고 새 포트를 포함하도록 브로드캐스트 및 멀티캐스트 패킷을 전달하는 흐름을 업데이트합니다. 또한 바인딩 테이블에 레코드를 생성하고 섀시를 식별하는 열을 제외한 모든 열을 채웁니다.
4. 모든 하이퍼바이저에서 ovn 컨트롤러는 논리적_을 수신합니다.플로우 테이블은 이전 단계에서 북으로 이동한 내용을 업데이트합니다. VIF를 소유한 VM의 전원이 꺼져 있는 한 ovn 컨트롤러는 많은 작업을 수행할 수 없습니다. 예를 들어 VIF가 실제로 어디에 존재하지 않기 때문에 VIF에서 패킷을 보내거나 받을 수 있도록 정렬할 수 없습니다.
5. 결국 사용자는 VIF를 소유하는 VM의 전원을 켭니다. VM 전원이 켜진 하이퍼바이저에서 하이퍼바이저와 Open vSwitch(문서/토픽/통합.rst에 설명됨) 간의 통합은 VIF를 OVN 통합 브리지에 추가하고 vif-id를 external_ids:iface-id에 저장하여 인터페이스가 새 VIF의 인스턴스화임을 나타냅니다(이 코드 중 새로운 VIF는 없음). OVS를 지원하는 하이퍼바이저에서 이미 수행된 기존 통합 작업입니다.)
6. VM의 전원이 켜진 하이퍼바이저에서 ovn 컨트롤러는 새 인터페이스의 external_ids:iface-id를 감지합니다. 이에 따라 OVN Southbound DB에서 iface-id 논리 포트를 iface-id에서 하이퍼바이저로 연결하는 행에 대한 바인딩 테이블의 섀시 열을 업데이트합니다. 그런 다음 ovn 컨트롤러가 로컬 하이퍼바이저의 OpenFlow 테이블을 업데이트하여 VIF로 송수신하는 패킷을 올바르게 처리합니다.
7. OpenStack을 비롯한 일부 CMS 시스템은 네트워킹이 준비된 경우에만 VM을 완전히 시작합니다. 이를 지원하기 위해 ovn-northd는 바인딩 테이블에서 행에 대해 업데이트된 섀시 열을 감지하고 OVN Northbound 데이터베이스의 Logical_Switch_Port 테이블에서 위쪽 열을 업데이트하여 VIF가 이제 작동 중임을 표시하여 이 열을 위로 밀어 올립니다. CMS가 이 기능을 사용하는 경우 VM의 실행을 허용하여 대응할 수 있습니다.
8. VIF가 상주하는 하이퍼바이저를 제외한 모든 하이퍼바이저에서 ovn 컨트롤러는 바인딩 테이블에서 완전히 채워진 행을 감지합니다. 이렇게 하면 논리적 포트의 물리적 위치가 OVN 컨트롤러에 제공되므로 각 인스턴스는 해당 스위치의 OpenFlow 테이블을 업데이트합니다(OVN DB Logical_의 논리적 데이터 경로 흐름에 기반함).플로우 테이블)을 사용하면 터널을 통해 VIF로 들어오고 나가는 패킷을 올바르게 처리할 수 있습니다.
9. 결국 사용자가 VIF를 소유하는 VM의 전원을 끕니다. VM의 전원이 꺼진 하이퍼바이저에서는 VIF가 OVN 통합 브리지에서 삭제됩니다.
10. VM의 전원이 꺼진 하이퍼바이저에서 ovn 컨트롤러는 VIF가 삭제되었음을 인식합니다. 이에 따라 논리적 포트의 바인딩 테이블에서 섀시 열 내용을 제거합니다.
11. 모든 하이퍼바이저에서 ovn 컨트롤러는 논리적 포트에 대한 바인딩 테이블 행의 빈 섀시 열을 감지합니다. 즉, ovn 컨트롤러가 더 이상 논리적 포트의 물리적 위치를 알 수 없으므로 각 인스턴스는 이를 반영하도록 OpenFlow 테이블을 업데이트합니다.
12. 결국 VIF(또는 VIF 전체 VM)가 더 이상 필요하지 않으면 관리자는 CMS 사용자 인터페이스 또는 API를 사용하여 VIF를 삭제합니다. CMS가 자체 구성을 업데이트합니다.
13. CMS 플러그인은 논리적_Switch_Port 테이블에서 해당 행을 삭제하여 OVN Northbound 데이터베이스에서 VIF를 제거합니다.
14. OVn-northd는 OVN Northerbound 업데이트를 수신하고 그에 따라 OVN Southbound 데이터베이스 Logical_에서 행을 제거하거나 업데이트하여 OVN Southbound 데이터베이스를 업데이트합니다.이제 파괴된 VIF와 관련된 흐름 테이블 및 바인딩 테이블입니다.
15. 모든 하이퍼바이저에서 ovn 컨트롤러는 논리적_을 수신합니다.플로우 테이블은 이전 단계에서 북으로 이동한 내용을 업데이트합니다. 이전 단계에서 VIF가 바인딩 테이블에서 제거되었을 때 이미 VIF에 연결할 수 없게 되었으므로 ovn 컨트롤러는 업데이트를 반영하도록 OpenFlow 테이블을 업데이트합니다.

***



### Life Cycle of a Container Interface Inside a VM

OVN은 OVN_NB 데이터베이스에 작성된 정보를 각 하이퍼바이저에서 OpenFlow로 변환하여 가상 네트워크 추상화를 제공합니다. OVN 컨트롤러가 Open vSwitch의 흐름을 수정할 수 있는 유일한 엔티티인 경우에만 멀티 테넌트용 보안 가상 네트워킹을 제공할 수 있습니다. Open vSwitch 통합 브리지가 하이퍼바이저에 상주하는 경우 VM 내에서 실행되는 테넌트 워크로드가 Open vSwitch 흐름을 변경할 수 없도록 하는 것이 적절한 가정입니다.

인프라 공급자가 컨테이너 내부의 애플리케이션을 신뢰하여 Open vSwitch 흐름을 수정하지 않는 경우 하이퍼바이저에서 컨테이너를 실행할 수 있습니다. 컨테이너가 VM 내에서 실행되고 OVN 컨트롤러에서 추가된 흐름이 있는 Open vSwitch 통합 브리지가 동일한 VM에 상주하는 경우에도 마찬가지입니다. 위의 두 경우 모두에서 워크플로는 이전 섹션("VIF의 수명 주기")의 예와 함께 설명되어 있는 것과 동일합니다.

이 섹션에서는 VM에 컨테이너가 생성되고 하이퍼바이저 내에 Open vSwitch 통합 브리지가 상주하는 경우 CIF(컨테이너 인터페이스)의 수명 주기에 대해 설명합니다. 이 경우 컨테이너 애플리케이션이 발생하더라도 VM 내에서 실행되는 컨테이너가 Open vSwitch 통합 브리지의 흐름을 수정할 수 없으므로 다른 테넌트는 영향을 받지 않습니다.

VM 내부에 여러 컨테이너가 생성되면 여러 CIF가 연결됩니다. 이러한 CIF와 연결된 네트워크 트래픽이 OVN용 하이퍼바이저에서 실행되는 Open vSwitch 통합 브리지에 도달해야 가상 네트워크 추상화를 지원할 수 있습니다. 또한 OVN은 서로 다른 CIF에서 발생하는 네트워크 트래픽을 구별할 수 있어야 합니다. CIF의 네트워크 트래픽을 구분하는 두 가지 방법이 있습니다.

한 가지 방법은 모든 CIF(1:1 모델)에 대해 하나의 VIF를 제공하는 것입니다. 즉, 하이퍼바이저에 많은 네트워크 디바이스가 있을 수 있습니다. 이는 모든 VIF를 관리하는 데 필요한 모든 추가 CPU 사이클로 인해 OVS 속도가 느려집니다. VM에서 컨테이너를 생성하는 엔티티도 하이퍼바이저에서 해당 VIF를 생성할 수 있어야 합니다.
두 번째 방법은 모든 CIF(1: 다수 모델)에 대해 단일 VIF를 제공하는 것입니다. 그러면 OVN은 모든 패킷에 기록된 태그를 통해 서로 다른 CIF에서 오는 네트워크 트래픽을 구별할 수 있습니다. OVN은 이 메커니즘을 사용하고 VLAN을 태그 메커니즘으로 사용합니다.

1. CIF의 수명 주기는 VM을 생성한 동일한 CMS 또는 해당 VM을 소유하는 테넌트 또는 VM을 처음 생성한 CMS와 다른 CMS에 의해 컨테이너가 VM 내부에 생성될 때 시작됩니다. 엔티티가 누구든 컨테이너 인터페이스의 네트워크 트래픽이 통과될 것으로 예상되는 VM의 네트워크 인터페이스와 연결된 vif-id를 알아야 합니다. 컨테이너 인터페이스를 생성하는 엔티티도 해당 VM 내에서 사용되지 않는 VLAN을 선택해야 합니다.

2. 컨테이너 산란 엔티티(직접 또는 기본 인프라를 관리하는 CMS를 통해)는 논리적_Switch_Port 테이블에 행을 추가하여 OVN Northbound 데이터베이스를 새 CIF를 포함하도록 업데이트합니다. 새 행에서 이름은 모든 고유 식별자이며, parent_name은 CIF의 네트워크 트래픽이 통과할 것으로 예상되는 VM의 vif-id이며 태그는 해당 CIF의 네트워크 트래픽을 식별하는 VLAN 태그입니다.
3. ovn-northd는 OVN Northbound 데이터베이스 업데이트를 수신합니다. 차례로 OVN Southbound 데이터베이스의 Logical_Flow 테이블에 행을 추가하여 새 포트를 반영하고 Binding 테이블에 새 행을 만들고 식별하는 열을 제외한 모든 열을 채움으로써 OVN Southbound 데이터베이스에 해당하는 업데이트를 수행합니다. 섀시.
4. 모든 하이퍼바이저에서 ovn 컨트롤러는 바인딩 테이블의 변경 사항을 구독합니다. 바인딩 테이블의 parent_port 열에 값을 포함하는 ovn-northd에 의해 새 행이 생성되면 OVN 통합 브리지가 external_ids:iface-id에서 vif-id에 동일한 값을 갖는 하이퍼바이저에서 ovn 컨트롤러가 로컬 하이퍼바이저의 OpenFlow 테이블을 업데이트하여 특정 VLAN 태그를 사용하여 VIF로 들어오고 나가는 패킷이 올바르게 h되도록 합니다.그리고 나서 실제 위치를 반영하도록 바인딩의 섀시 열을 업데이트합니다.
5. 기본 네트워크가 준비된 후에만 컨테이너 내에서 응용 프로그램을 시작할 수 있습니다. 이를 지원하기 위해 ovn-northd는 바인딩 테이블에서 업데이트된 섀시 열을 감지하고 OVN Northbound 데이터베이스의 Logical_Switch_Port 테이블에서 위쪽 열을 업데이트하여 CIF가 작동 중임을 나타냅니다. 컨테이너 응용 프로그램을 시작할 책임이 있는 엔티티가 이 값을 쿼리하고 응용 프로그램을 시작합니다.
6. 결국 컨테이너를 만들고 시작한 엔티티가 컨테이너를 중지합니다. 엔티티는 CMS(또는 직접)를 통해 Logical_Switch_Port 테이블에서 해당 행을 삭제합니다.
7. OVn-northd는 OVN Northerbound 업데이트를 수신하고 그에 따라 OVN Southbound 데이터베이스 Logical_에서 행을 제거하거나 업데이트하여 OVN Southbound 데이터베이스를 업데이트합니다.현재 파괴된 CIF와 관련된 플로우 테이블입니다. 또한 해당 CIF에 대한 바인딩 테이블의 행도 삭제합니다.
8. 모든 하이퍼바이저에서 ovn 컨트롤러는 논리적_을 수신합니다.플로우 테이블은 이전 단계에서 북으로 이동한 내용을 업데이트합니다. ovn 컨트롤러는 업데이트를 반영하도록 OpenFlow 테이블을 업데이트합니다.

***

### Architectural Physical Life Cycle of a Packet

이 섹션에서는 OVN을 통해 한 가상 시스템 또는 컨테이너에서 다른 가상 시스템으로 이동하는 방법에 대해 설명합니다.
이 설명은 패킷의 물리적 처리에 초점을 맞춥니다. 패킷의 논리적 수명 주기에 대한 설명은 Logical_을 참조하십시오.흐름 테이블은 ovn-sb(5)입니다.

이 섹션에서는 몇 가지 데이터 및 메타데이터 필드에 대해 자세히 설명합니다.



* tunnel key
  * OVN이 패킷을 Geneve 또는 다른 터널에서 캡슐화할 때 수신 OVN 인스턴스가 패킷을 올바르게 처리할 수 있도록 추가 데이터를 패킷에 첨부합니다. 이것은 특정 캡슐화에 따라 다른 형태를 취하지만, 각각의 경우에 우리는 이것을 "터널 키"라고 부릅니다. 자세한 내용은 아래의 터널 캡슐화를 참조하십시오.

* logical datapath field
  * 패킷이 처리되는 논리적 데이터 경로를 나타내는 필드입니다. OVN은 OpenFlow 1.1+가 "metadata"를 호출하여 논리적 데이터 경로를 저장하는 필드를 사용합니다(이 필드는 터널 키의 일부로 터널을 가로질러 전달됩니다).
* logical input port field
  * 패킷이 논리적 데이터 경로로 들어간 논리적 포트를 나타내는 필드입니다. OVN은 이를 Open vSwitch 확장 레지스터 번호 14에 저장합니다.
    Genve 및 STT 터널은 터널 키의 일부로 이 필드를 통과합니다. VXLAN 터널에 논리적 입력 포트가 명시적으로 전달되지 않지만 OVN은 VXLAN을 사용하여 OVN의 관점에서만 단일 논리적 포트로 구성된 게이트웨이와 통신하므로 OVN은 논리적 입력 포트 필드를 OVN 논리적 파이프라인에 수신할 때 이 경로로 설정할 수 있습니다.
* logical output port field
  * 패킷이 논리적 데이터패스를 떠나는 논리적 포트를 나타내는 필드입니다.
    논리적 수신 파이프라인이 시작될 때 0으로 초기화됩니다. OVN은 이를 Open vSwitch 확장 레지스터 15에 저장합니다. 
    Genve 및 STT 터널은 터널 키의 일부로 이 필드를 통과합니다.
    VXLAN 터널은 논리적 출력 포트 필드를 전송하지 않습니다. VXLAN 터널은 터널 키에 논리적 출력 포트 필드를 전송하지 않기 때문에 OVN 하이퍼바이저에 의해 VXLAN 터널에서 패킷이 수신되면 패킷이 테이블 8에 다시 제출되어 출력 포트를 결정합니다. 패킷이 테이블 32에 도달하면 이러한 패킷이 MLF_RCV_FROM_VX를 확인하여 로컬 전송을 위해 테이블 33에 다시 제출됩니다. 이는 패킷이 VXLAN 터널에서 도착할 때 설정됩니다.

* conntrack zone field for logical ports
  * 논리적 포트의 연결 추적 영역을 나타내는 필드입니다. 값은 로컬 유의성만 가지며 섀시 간에는 유의하지 않습니다. 논리적 수신 파이프라인이 시작될 때 0으로 초기화됩니다. OVN은 이를 Open vSwitch 확장 레지스터 번호 13에 저장합니다.
* conntrack zone fields for routers
  * 라우터의 연결 추적 영역을 나타내는 필드입니다. 이러한 값은 로컬 유의성만 가지며 섀시 간에는 유의하지 않습니다. OVN은 Open vSwitch 확장 레지스터 번호 11에 DNATing에 대한 영역 정보를 저장하고 Open vSwitch 확장 레지스터 번호 12에 SNAT에 대한 영역 정보를 저장합니다.

* logical flow flags
  * 논리 플래그는 후속 테이블에서 일치되는 규칙을 결정하기 위해 테이블 간의 컨텍스트 유지를 처리하기 위한 것입니다. 이러한 값은 로컬 유의성만 가지며 섀시 간에는 유의하지 않습니다. OVN은 논리적 플래그를 Open vSwitch 확장 레지스터 번호 10에 저장합니다.
* VLAN ID
  * VLAN ID는 OVN과 VM 내부에 중첩된 컨테이너 간의 인터페이스로 사용됩니다(위의 자세한 내용은 VM 내부의 컨테이너 인터페이스 수명 주기 참조).

***

처음에 수신 하이퍼바이저의 VM 또는 컨테이너는 OVN 통합 브리지에 연결된 포트로 패킷을 전송합니다.

1.  OpenFlow 테이블 0은 물리적/논리적 변환을 수행합니다. 패킷의 수신 포트와 일치합니다. 해당 작업은 패킷이 통과하고 있는 논리적 데이터 경로를 식별하도록 논리적 데이터패스 필드를 설정하고 수신 포트를 식별하는 논리적 입력 포트 필드를 설정하여 논리적 메타데이터로 패킷에 주석을 추가합니다. 그런 다음 테이블 8에 다시 제출하여 논리적 수신 파이프라인을 입력합니다.
   VM에 중첩된 컨테이너에서 생성된 패킷은 약간 다른 방식으로 처리됩니다. 원래 컨테이너는 VIF별 VLAN ID를 기반으로 구분할 수 있으므로 물리적 변환 흐름과 논리적 변환 흐름이 추가로 VLAN ID와 일치하고 작업은 VLAN 헤더를 분리합니다. 이 단계를 수행한 후 OVN은 컨테이너의 패킷을 다른 패킷과 마찬가지로 처리합니다.
   표 0은 다른 섀시에서 들어오는 패킷도 처리합니다. 터널인 수신 포트를 통해 다른 패킷과 구분합니다. 패킷이 OVN 파이프라인에 막 진입하는 경우와 마찬가지로 이 작업은 논리적 데이터 경로 및 논리적 수신 포트 메타데이터로 이러한 패킷에 주석을 추가합니다. 또한 작업은 논리적 출력 포트 필드를 설정합니다. 논리적 출력 포트가 알려진 후 OVN 터널링에서 발생하기 때문에 이 필드를 사용할 수 있습니다. 이 세 가지 정보는 터널 캡슐화 메타데이터에서 가져옵니다(인코딩에 대한 자세한 내용은 터널 캡슐화 참조). 그런 다음 작업이 테이블 33에 다시 제출되어 논리적 송신 파이프라인을 입력합니다.'

2. OpenFlow 테이블 8 ~ 31은 Logical_에서 논리적 수신 파이프라인을 실행합니다.OVN Southbound 데이터베이스의 흐름 테이블입니다. 이러한 표는 논리적 포트 및 논리적 데이터패스와 같은 논리적 개념의 측면에서 전체적으로 표현됩니다. ovn 컨트롤러의 작업 중 큰 부분은 이들을 동등한 OpenFlow로 변환하는 것입니다(특히 표 번호를 변환합니다. 
   논리적 입니다.흐름 테이블 0 ~ 23은 OpenFlow 테이블 8 ~ 31이 됩니다. 각 논리적 흐름은 하나 이상의 OpenFlow 흐름에 매핑됩니다. 실제 패킷은 일반적으로 이들 중 하나만 일치하지만, 경우에 따라 이러한 흐름 중 하나 이상과 일치할 수 있습니다(모두 동일한 작업을 가지고 있기 때문에 문제가 되지 않습니다. ovn 컨트롤러는 논리적 흐름의 UUID 중 처음 32비트를 OpenFlow 흐름 또는 흐름의 쿠키로 사용합니다. (논리 흐름의 UUID 중 처음 32비트가 반드시 고유하지는 않으므로, 반드시 고유하지는 않습니다.) 
   일부 논리적 흐름은 Open vSwitch "consolidive match" 확장에 매핑될 수 있습니다(7)(vms-fields(7) 참조). 연결 동작이 있는 흐름은 여러 논리적 흐름에 해당될 수 있으므로 0의 OpenFlow 쿠키를 사용합니다. 결점 일치에 대한 OpenFlow 흐름에는 conj_id에 대한 일치가 포함됩니다. 
   일부 논리적 흐름은 해당 하이퍼바이저에서 사용할 수 없는 경우 해당 하이퍼바이저의 OpenFlow 테이블에 표시되지 않을 수 있습니다. 예를 들어, 논리적 스위치의 VIF가 지정된 하이퍼바이저에 상주하지 않고 해당 하이퍼바이저에서 논리적 스위치와 라우터를 통한 홉의 오버라이즈(예: 하이퍼바이저의 VIF에서 시작되는 오버라이즈)에 논리적 흐름을 표시할 수 없는 경우 논리적 흐름이 여기에 표시되지 않을 수 있습니다.

   대부분의 OVN 작업은 OpenFlow(예: OVS 확장이 있는 경우)에서 상당히 명확한 구현을 수행하며, 다음으로는 resubmit, field = constant, set_field로 구현됩니다.afew는 더 자세히 설명할 가치가 있습니다.

output:

* 패킷을 테이블 32에 다시 제출하여 구현합니다. 파이프라인이 두 개 이상의 출력 작업을 실행하는 경우 각 출력 작업은 별도로 테이블 32에 다시 제출됩니다. 이를 통해 여러 개의 패킷 복사본을 여러 포트로 전송할 수 있습니다. (패킷이 출력 작업 간에 수정되지 않고 일부 복사본이 동일한 하이퍼바이저로 전송되는 경우 논리적 멀티캐스트 출력 포트를 사용하면 하이퍼바이저 간의 대역폭이 절약됩니다.)

get_arp(P, A); 

get_nd(P, A);

* 인수를 OpenFlow 필드에 저장한 다음 표 66에 다시 제출하여 구현되며, 표 66은 OVN Southbound 데이터베이스의 MAC_Binding 테이블에서 생성된 흐름으로 ovn 컨트롤러가 채웁니다. 표 66에 일치 항목이 있는 경우 해당 작업은 바인딩된 MAC를 이더넷 대상 주소 필드에 저장합니다. (OpenFlow 작업은 인수에 사용되는 OpenFlow 필드를 저장 및 복원하므로 OVN 작업이 이 임시 사용을 인식할 필요가 없습니다.)

put_arp(P, A, E); 

put_nd(P, A, E);

* 인수를 OpenFlow 필드에 저장한 다음 ovn 컨트롤러로 패킷을 출력하여 구현하며, 이 경우 MAC_Binding 테이블이 업데이트됩니다.
  (OpenFlow 작업은 인수에 사용되는 OpenFlow 필드를 저장 및 복원하므로 OVN 작업이 이 임시 사용을 인식할 필요가 없습니다.)

R = lookup_arp(P, A, M); 

R = lookup_nd(P, A, M);

* 인수를 OpenFlow 필드에 저장한 다음 표 67에 다시 제출하여 구현됩니다. 표 67은 OVN Southbound 데이터베이스의 MAC_Binding 테이블에서 생성된 흐름으로 채워집니다. 표 67에 일치 항목이 있는 경우 해당 작업은 논리 흐름 플래그 MLF_LOOKUP_MAC를 설정합니다.
  (OpenFlow 작업은 인수에 사용되는 OpenFlow 필드를 저장 및 복원하므로 OVN 작업이 이 임시 사용을 인식할 필요가 없습니다.)



3. OpenFlow 테이블 32 ~ 47은 논리적 수신 파이프라인에서 출력 작업을 구현합니다. 
   특히 테이블 32는 원격 하이퍼바이저에 대한 패킷을 처리하고 테이블 33은 로컬 하이퍼바이저에 대한 패킷을 처리하며 테이블 34는 논리적 수신 포트와 송신 포트가 동일한 패킷을 삭제해야 하는지 여부를 확인합니다.
   논리 패치 포트는 특별한 경우입니다. 논리적 패치 포트는 물리적 위치가 없으며 모든 하이퍼바이저에 실제로 위치합니다. 따라서 로컬 하이퍼바이저의 포트에 대한 출력용 플로우 테이블 33은 유니캐스트 논리 패치 포트에도 출력을 자연스럽게 구현합니다. 그러나 멀티캐스트 그룹의 논리 포트를 포함하는 각 하이퍼바이저도 패킷을 논리 패치 포트로 출력하기 때문에 논리 멀티캐스트 그룹의 일부인 논리 패치 포트에 동일한 로직을 적용하면 패킷 복제가 발생합니다. 따라서 멀티캐스트 그룹은 테이블 32의 논리 패치 포트에 대한 출력을 구현합니다.
   테이블 32의 각 흐름은 원격 하이퍼바이저의 논리적 포트를 포함하는 유니캐스트 또는 멀티캐스트 논리 포트의 논리적 출력 포트에서 일치합니다. 각 흐름의 작업은 일치하는 포트로 패킷을 보내는 것을 구현합니다. 원격 하이퍼바이저의 유니캐스트 논리 출력 포트의 경우 작업은 터널 키를 올바른 값으로 설정한 다음 터널 포트의 패킷을 올바른 하이퍼바이저로 보냅니다. (원격 하이퍼바이저가 패킷을 수신하면 테이블 0이 패킷을 터널링된 패킷으로 인식하여 테이블 33에 전달합니다.) 멀티캐스트 논리 출력 포트의 경우 작업은 유니캐스트 대상과 동일한 방식으로 각 원격 하이퍼바이저에 패킷 복사본을 보냅니다. 멀티캐스트 그룹에 로컬 하이퍼바이저의 논리 포트 또는 포트가 포함된 경우 해당 작업도 표 33에 다시 제출됩니다. 표 32에는 다음도 포함됩니다.